
# Primus Patch Notes & Extended Argument Documentation

This document records the modifications made to integrate and extend **Megatron** and **TorchTitan** within the Primus framework via patching. It includes new arguments introduced for configuration and highlights the affected code paths.

---

## Sections

- [Primus Patch Notes \& Extended Argument Documentation](#primus-patch-notes--extended-argument-documentation)
  - [Sections](#sections)
  - [1. Base Module Parameters](#1-base-module-parameters)
  - [2. Megatron Patch Summary](#2-megatron-patch-summary)
    - [2.1 Module-Level Parameters](#21-module-level-parameters)
    - [2.2 Model-Definition Parameters](#22-model-definition-parameters)
  - [3. TorchTitan Patch Summary](#3-torchtitan-patch-summary)

---

## 1. Base Module Parameters
The following arguments are defined in the base module configuration file:
[primus/configs/modules/module_base.yaml](https://github.com/AMD-AIG-AIMA/Primus/blob/main/primus/configs/modules/module_base.yaml)
This base config is inherited by all other modules in the framework, so every module supports these parameters. These options control whether a module participates in training and how its logging behaves.

| Argument Name       | Default Value | Description                                                                                |
| ------------------- | ------------- | ------------------------------------------------------------------------------------------ |
| `trainable`         | `false`       | Whether the module is trainable.                                                           |
| `sink_level`        | `null`        | Global sink level for logging. Overrides `file_sink_level` and `stderr_sink_level` if set. |
| `file_sink_level`   | `DEBUG`       | Logging level for file sink (e.g., log file output).                                       |
| `stderr_sink_level` | `INFO`        | Logging level for standard error (console) output.                                         |

---


## 2. Megatron Patch Summary

### 2.1 Module-Level Parameters

These arguments are introduced in the Megatron module logic (e.g., training loop, logging, resume logic). They are defined via patching and can be configured to control training behavior and logging utilities.

| New Argument                         | Default Value | Version | Description                                                                                    | Patched Files                                                                                                                                                                                                                                                                                                | Notes                                            |
| ------------------------------------ | ------------- | ------- | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------ |
| `disable_tensorboard`                | `true`        | v0.1.0  | Whether to disable TensorBoard. Set to `false` if you want to enable profiling or torch trace. | NA                                                                                                                                                                                                                                                                                                           | Required for timeline and performance debugging. |
| `disable_wandb`                      | `true`        | v0.1.0  | Whether to disable Weights & Biases logging.                                                   | NA                                                                                                                                                                                                                                                                                                           | Useful for internal benchmarking.                |
| `disable_compile_dependencies`       | `true`        | v0.1.0  | Disables Megatronâ€™s custom kernel compilation. Most ops are already covered by TE.             | NA                                                                                                                                                                                                                                                                                                           | Avoids redundant compilation steps.              |
| `auto_continue_train`                | `false`       | v0.1.0  | Automatically resume training from the latest checkpoint if found in the `--save` path.        | NA                                                                                                                                                                                                                                                                                                           | Simplifies job restarts.                         |
| `disable_last_saving`                | `false`       | v0.1.0  | Skip saving the final checkpoint at the last iteration.                                        | NA                                                                                                                                                                                                                                                                                                           | Useful for profiling or benchmarking runs.       |
| `no_fp8_weight_transpose_cache`      | `false`       | v0.2.0  | Disable the FP8 weight transpose cache to save memory.                                         | `megatron.core.extensions.transformer_engine.TELinear`, `megatron.core.extensions.transformer_engine.TELayerNormColumnParallelLinear`, `megatron.core.extensions.transformer_engine.TEDelayedScaling`                                                                                                        | May affect performance but reduce memory use.    |
| `decoder_pipeline_manual_split_list` | `null`        | v0.2.0  | Enable manual pipeline split in (interleaved) 1F1B pipeline parallelism.                       | `megatron.core.transformer.transformer_block.get_num_layers_to_build`, `megatron.core.transformer.transformer_layer.get_transformer_layer_offset`                                                                                                                                                            | May be deprecated when megatron gets updated.    |
| `pp_warmup`                          | `false`       | v0.2.0  | Add fwd/bwd warmup to save iter1's time when pp degree is large.                             | NA                                                                                                                                                                                                                                                                                                           | Can save much time for pipeline debug.           |
| `dump_pp_data`                       | `false`       | v0.2.0  | Enable dumping pp schedule data for visualization.                                             | `megatron.core.pipeline_parallel.schedules.forward_step`, `megatron.core.pipeline_parallel.schedules.backward_step`, `megatron.core.pipeline_parallel.schedules.forward_backward_pipelining_with_interleaving`, `megatron.core.pipeline_parallel.schedules.forward_backward_pipelining_without_interleaving` | Useful for pipeline schedule visualization.      |
| `disable_profiler_activity_cpu`                | `false`       | v0.2.0  | Disable CPU activity in torch profiling.                                        |  NA                                                                                                                                                                                                                                                                                                         | If you only want to trace CUDA kernels and get a smaller trace JSON file, you can enable this option. However, if you plan to run with TraceLen, please do not enable it.  more torch profiler args: <br>`torch_profiler_record_shapes: true`, <br>`torch_profiler_with_stack: true`, <br>`torch_profiler_use_gzip: true`      |
| `use_rocm_mem_info`                        | `false`       | v0.2.0  | Logging ROCm memory information in Megatron-LM Trainer                             | NA                                                                                                                                                                                                                                                                                                           | If `use_rocm_mem_info = True`, ROCm memory information will be collected with `rocm-smi` at every iteration.           |
| `use_rocm_mem_info_iters`                        | `[1,2]`       | v0.2.0  | Logging ROCm memory information in Megatron-LM Trainer for some iterations                            | NA                                                                                                                                                                                                                                                                                                           | If `use_rocm_mem_info = False`, ROCm memory information will be collected at the iterations specified in `use_rocm_mem_info_iters`.           |
| `patch_zero_bubble`                        | `false`       | v0.2.0  | Using Zero-Bubble pipeline parallism  | `megatron.core.optimizer.ChainedOptimizer`, `megatron.core.pipeline_parallel.get_forward_backward_func`, `megatron.core.tensor_parallel.layers.LinearWithGradAccumulationAndAsyncCommunication`, `megatron.core.parallel_stat.default_embedding_ranks`, `megatron.core.parallel_stat.is_pipeline_last_stage`, `megatron.core.parallel_stat.is_rank_in_embedding_group`, `megatron.core.distributed.finalize_model_grads`, `megatron.core.transformer.transformer_layer.get_transformer_layer_offset` | If `patch_zero_bubble = True`, Zero bubble pipeline parallism will be enable to use. See more detail at [ZeroBubble User Guide](./backends/megatron/core/pipeline_parallel/zerobubble/README.md)           |
| `disable_mlflow`                        | `true`       | v0.3.0  | Track model development using MLflow                             | NA                                                                                                                                                                                                                                                                                                           |  Envs: <br> `export DATABRICKS_TOKEN=your_token`<br>`export DATABRICKS_HOST=your_host`<br>`export MLFLOW_TRACKING_URI=databricks`<br>`export MLFLOW_REGISTRY_URI=databricks-uc`<br>Arguments: <br>`mlflow_run_name: null`,<br>`mlflow_experiment_name: null`     |

---

### 2.2 Model-Definition Parameters

These arguments affect the internal architecture or layer implementations. They are patched into the model construction logic and used for tuning or debugging specific variants.

| New Argument                        | Default Value | Version | Description                                                               | Patched Files                                                                                                                                                                                                                                                                                                                   | Notes                                       |
| ----------------------------------- | ------------- | ------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------- |
| `disable_primus_topk_router`   | `false`       | v0.1.0  | Disable PrimusTopkRouter and use TopkRouter implemented by megatron. | `megatron.core.transformer.moe.router.TopKRouter`                                                                                                                                                                                                                                                                               | Used to debug internal.         |
| `moe_router_force_load_balancing`   | `false`       | v0.1.0  | Force token redistribution in MoE to achieve load balance across experts. | `megatron.core.transformer.moe.router.TopKRouter`                                                                                                                                                                                                                                                                               | Use to debug MoE imbalance issues.          |
| `use_deprecated_20241209_moe_layer` | `false`       | v0.1.0  | Enable legacy MoE implementation for debugging/perf comparison.           | `megatron.core.transformer.moe.moe_layer.MoELayer`, `megatron.core.transformer.moe.moe_layer.MoESubmodules`, `megatron.core.transformer.moe.experts.GroupedMLP`, `megatron.core.transformer.moe.experts.SequentialMLP`, `megatron.core.transformer.moe.experts.TEGroupedMLP`, `megatron.core.transformer.moe.router.TopKRouter` | Deprecated, used for internal testing only. |
| `moe_permute_fusion`   | `false`       | v0.1.0  | Permutation and unpermutation fusion. | `megatron.core.extensions.transformer_engine`, `megatron.core.transformer.moe.moe_utils`                                                                                                                                                                                                                                                                               | Fuse permutation and unpermutation in moe layer.         |
| `fused_padded_mla_attention`   | `false`       | v0.1.0  | Pad the V head dim to match the Q head dim. | `megatron.core.transformer.multi_latent_attention.PaddedMLASelfAttention`                                                                                                                                                                                                                                                                               | To enable fused attention and reduce memory usage, this module pads the V tensor so that all Q, K, and V have a uniform head dimension of 192. After padding, AMD TE's fused attention can be invoked, resulting in more efficient memory usage and improved performance..         |
| `enable_primus_turbo`   | `false`       | v0.2.0  | Use Primus turbo as backend. | `megatron.core.models.gpt.gpt_layer_specs.TEDotProductAttention`, `megatron.core.models.gpt.gpt_layer_specs.PrimusTurboRowParallelLinear`, `megatron.core.models.gpt.gpt_layer_specs.TELayerNormColumnParallelLinear`, `megatron.core.models.gpt.gpt_layer_specs.TEColumnParallelLinear`, `megatron.core.models.gpt.gpt_model.tensor_parallel.ColumnParallelLinear`, `megatron.core.models.gpt.moe_module_specs.GroupedMLP`, `megatron.core.models.gpt.moe_module_specs.TEColumnParallelLinear`, `megatron.core.models.gpt.moe_module_specs.TERowParallelLinear`                                                                                                                                                                                                                                                                           | Used to accelerate training. See fine-grained control flags in primus-turbo.yaml        |
| `moe_use_fused_router_with_aux_score`   | `false`       | v0.2.0  | Fused router topk and calculation of moe aux loss score. Need Primus turbo backend | `megatron.core.transformer.moe.router.TopKRouter`                                                                                                                                                                                                                                                                               | Used to reduce launch overhead of the small kernels in router.         |
| `use_turbo_deepep`   | `false`       | v0.4.0  | Use Primus-turbo `DeepEPTokenDispatcher`. | `megatron.core.transformer.moe.token_dispatcher.MoEFlexTokenDispatcher`                                                                                                                                                                                                                                                              | Used Primus-Turbo DeepEP to accelerate MoE token dispatcher.  **You must both set`enable_primus_turbo=True` and `use_turbo_deepep=True` to enable this function.**         |
| `turbo_deepep_num_cu`   | `32`       | v0.4.0  | Set the number of CUs to use for Primus-Turbo DeepEP. |   | 64 or 80 for ep8, 32 for ep16-64 is best practice.  |
| `turbo_deepep_use_comm_stream`   | `false`       | v0.4.0  | Primus-Turbo DeepEP will use an internal stream to dispatch/combine when enabled, default used `current_stream` |   |  **Please both set`enable_primus_turbo=True` and `use_turbo_deepep=True` first**
| `turbo_sync_free_moe_stage`   | `0`       | v0.4.0  | Primus Sync-Free MoE has 4 stages. See [RFC: Primus-Megatron SyncFree MoE](https://github.com/AMD-AGI/Primus/issues/203) for more details. |   |   stage 2 is recommended for better performance. **Please set`enable_primus_turbo=True` first**   |


---

## 3. TorchTitan Patch Summary

| New Argument | Default Value | Version | Description | Patched Files | Notes |
| ------------ | ------------- | ------- | ----------- | ------------- | ----- |
| `ABC`        | `true`        | v0.1.0  | ABC         | `abc.py`      | ABC   |

---
