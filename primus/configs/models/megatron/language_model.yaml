includes:
  - primus_megatron_model.yaml


# megatron model arguments include:
# architecture/tokenizer/init/mix-precision/fusion/moe/model-parallel/Optimizations

# model architecture
use_legacy_models: false
deprecated_use_mcore_models: false
num_layers: 24
encoder_num_layers: null
decoder_num_layers: null
hidden_size: 1024
num_attention_heads: 16
attention_backend: auto
group_query_attention: false
qk_layernorm: false
qk_l2_norm: false
num_query_groups: null
add_position_embedding: false
position_embedding_type: learned_absolute
max_position_embeddings: null
untie_embeddings_and_output_weights: true

ffn_hidden_size: null
kv_channels: null
hidden_dropout: 0.1
attention_dropout: 0.1
fp32_residual_connection: false

apply_residual_connection_post_layernorm: false
add_bias_linear: false
add_qkv_bias: false
swiglu: true
quick_geglu: false
openai_gelu: false
squared_relu: false
rotary_base: 10000
rotary_percent: 1.0
rotary_interleaved: false
rotary_seq_len_interpolation_factor: null
use_rotary_position_embeddings: null
use_rope_scaling: false # Apply rope scaling as used in llama3.x
rope_scaling_factor: 8.0 # float, Rope scaling factor in llama3.x models
transformer_impl: transformer_engine

rope_type: null # rope, yarn

# tokenizer
# 'BertWordPieceLowerCase', 'BertWordPieceCase', 'GPT2BPETokenizer',
# 'SentencePieceTokenizer', 'GPTSentencePieceTokenizer', 'HuggingFaceTokenizer',
# 'Llama2Tokenizer', 'TikTokenizer', 'MultimodalTokenizer', 'NullTokenizer',
# 'NullMultimodalTokenizer'
# 'DeepSeekV2Tokenizer', 'DeepSeekV3Tokenizer'
tokenizer_type: null # str [primus]
tokenizer_model: null
vocab_size: null # int
vocab_file: null # str
vocab_extra_ids: 0 # int
tiktoken_pattern: null # str
tiktoken_num_special_tokens: 1000 # int
tiktoken_special_tokens: null # str
legacy_tokenizer: false
trust_remote_code: false

# initialization
init_method_std: 0.02

# mixed-precision
apply_query_key_layer_scaling: false
attention_softmax_in_fp32: false

# fusion
bias_gelu_fusion: true
cross_entropy_loss_fusion: False
cross_entropy_fusion_impl: "native" # "native", "te"
bias_swiglu_fusion: true
masked_softmax_fusion: true
no_persist_layer_norm: false
bias_dropout_fusion: true
apply_rope_fusion: true

# miscellaneous
clone_scatter_output_in_embedding: true

norm_epsilon: 1.0e-05
normalization: "LayerNorm" # alt value supported by TE: "RMSNorm"
apply_layernorm_1p: false

# MLA
multi_latent_attention: false
q_lora_rank: null # int
kv_lora_rank: 32 # int
qk_head_dim: 128 # int
qk_pos_emb_head_dim: 64 # int
v_head_dim: 128 # int
rotary_scaling_factor: 1.0 # float
mscale: 1.0 # float
mscale_all_dim: 1.0 # float

# MTP
mtp_num_layers: null # int
mtp_loss_scaling_factor: 0.1 # float

# MoE related
num_experts: null
moe_layer_freq: 1 # int
moe_ffn_hidden_size: null # int
moe_shared_expert_overlap: false
moe_shared_expert_intermediate_size: null # int
moe_grouped_gemm: false
# router arguments
moe_router_load_balancing_type: "aux_loss" # 'aux_loss', 'seq_aux_loss', 'sinkhorn', 'none'
moe_router_dtype: null # str, fp32, fp64
moe_router_score_function: softmax # softmax, sigmoid
moe_router_topk: 2
moe_router_pre_softmax: false
# Number of groups to divide experts into for group-limited routing. When using group-limited routing:
# 1) Experts are divided into equal-sized groups,
# 2) For each token, a subset of groups are selected based on routing scores (sum of top-2 expert scores within each group),
# 3) From these selected groups, moe_router_topk experts are chosen.
# Two common use cases:
# 1) Device-limited routing: Set equal to expert parallel size (EP) to limit each token to experts on a subset of devices
#   (See DeepSeek-V2: https://arxiv.org/pdf/2405.04434)
# 2) Node-limited routing: Set equal to number of nodes in EP group to limit each token to experts on a subset of nodes
#   (See DeepSeek-V3: https://arxiv.org/pdf/2412.19437)')
moe_router_num_groups: null # int
moe_router_group_topk: null # int
moe_router_topk_scaling_factor: null # float
moe_router_enable_expert_bias: false
moe_router_bias_update_rate: 1.0e-03
moe_use_legacy_grouped_gemm: false
moe_aux_loss_coeff: 0.0 # 1.0e-02 would be a good start value for load balance loss.
moe_z_loss_coeff: null # 1.0e-03 would be a good start value for z-loss
moe_input_jitter_eps: null
moe_token_dispatcher_type: allgather # str: 'allgather', 'alltoall', 'flex', 'alltoall_seq'
moe_enable_deepep: false
moe_per_layer_logging: false
moe_expert_capacity_factor: null
moe_pad_expert_input_to_capacity: false
moe_token_drop_policy: probs
moe_layer_recompute: false
moe_extended_tp: false
moe_use_upcycling: false
moe_permute_fusion: false
delay_wgrad_compute: false

# Model parallelism
model_parallel_size: null
tensor_model_parallel_size: 1
encoder_tensor_model_parallel_size: 0
context_parallel_size: 1
cp_comm_type: p2p # p2p, a2a, allgather or a2a+p2p
hierarchical_context_parallel_sizes: null

pipeline_model_parallel_size: 1
pipeline_model_parallel_layout: null
pipeline_model_parallel_comm_backend: null # str: nccl, ucc
encoder_pipeline_model_parallel_size: 0
pipeline_model_parallel_split_rank: null
decoder_first_pipeline_num_layers: null # int
decoder_last_pipeline_num_layers: null # int
num_layers_per_virtual_pipeline_stage: null # int
num_virtual_stages_per_pipeline_rank: null # int
microbatch_group_size_per_vp_stage: null # int
sequence_parallel: true
expert_model_parallel_size: 1
expert_tensor_parallel_size: null # int

high_priority_stream_groups: []         # List of group names to assign high priority
use_te_activation_func: false           # Whether to use Transformer Engine's activation kernel

# Initialization
perform_initialization: true
use_cpu_initialization: null

# Optimizations
gradient_accumulation_fusion: true
async_tensor_model_parallel_allreduce: true
tp_comm_overlap: false
tp_comm_overlap_cfg: null

# Debug Options
tp_comm_overlap_ag: true
tp_comm_overlap_rs: true
tp_comm_overlap_rs_dgrad: False
tp_comm_split_ag: true
tp_comm_split_rs: true
tp_comm_bulk_wgrad: true
tp_comm_bulk_dgrad: true

# CPU Offloading
# Timing
barrier_with_L1_time: true

tp_comm_bootstrap_backend: nccl
