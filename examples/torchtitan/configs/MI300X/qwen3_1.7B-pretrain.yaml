work_group: ${PRIMUS_TEAM:amd}
user_name: ${PRIMUS_USER:root}
exp_name: ${PRIMUS_EXP_NAME:qwen3_32B-pretrain}
workspace: ./output

modules:
  pre_trainer:
    framework: torchtitan
    config: pre_trainer.yaml

    # model to run
    model: qwen3_1.7b.yaml
    overrides:

      optimizer:
        name: "AdamW"
        lr: 8.0e-4
        eps: 1.0e-8

      lr_scheduler:
        warmup_steps: 20  # lr scheduler warm up, 20% total steps

      training:
        local_batch_size: 4
        seq_len: 4096
        max_norm: 1.0    # grad norm clipping
        steps: 50

      parallelism:
        data_parallel_replicate_degree: 1
        data_parallel_shard_degree: -1
        fsdp_reshard_after_forward: "default"  # default / never / always
        tensor_parallel_degree: 1
        context_parallel_degree: 1

      checkpoint:
        enable: false
        folder: "checkpoint"
        interval: 500
        last_save_model_only: false
        export_dtype: "float16"
        async_mode: "disabled"  # ["disabled", "async", "async_with_pinned_mem"]

      # quantize:
      #   linear:
      #     float8:
      #       enable_fsdp_float8_all_gather: false
      #       precompute_float8_dynamic_scale_for_fsdp: false
      #       filter_fqns:
      #         - "output"

      primus_turbo:
        enable_primus_turbo: true
        enable_attention_float8: false
